---
title: 海量数据处理场景
date: 2023-09-10 21:01:55
tags: 场景题
---
# 大文件找top100单词

### 题目描述

有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

### 解答思路

由于内存限制，我们无法直接将大文件所有词一次性全部读到内存中。因此，可以采用分治策略，把一个大文件分解成n个小文件，保证每个文件的大小小于内存大小限制（这里是 1MB），进而直接将单个小文件读取到内存中进行处理。
**1.哈希取余**
遍历大文件的每个词word，执行 hash(word) % 5000，将结果为 i 的词存放到文件 file(i) 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解，直到小于1MB为止。
**2.统计频数**
接着统计每个小文件 file(i) 中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。遍历到词 word，如果在 HashMap 中不存在，则执行 map.put(word, 1)；存在则执行 map.put(word, map.get(word)+1)，将该词频数加 1。
**3.小顶堆**
接下来，我们构建一个堆大小为 100 的小顶堆来找出所有词中出现频数最高的 top100 单词。依次遍历每个小文件，遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后调整小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。
**注意：若要找出最小top100单词的方法与之类似，将小顶堆换成大顶堆就好了。**
从序列中选择最大的K个数（小顶堆）
场景：要选择最大的k个数。
做法：【构造小顶堆（一共k个数）】，每次取数组中剩余数与 【堆顶的元素（k个数中的最小值）】 进行比较。如果新数比堆顶元素大，则删除堆顶元素，并添加这个新数到堆中；如果新数比堆顶元素小，则说明堆中的k个数都比新数大，所以不用进行操作。
从序列中选择最小的K个数（大顶堆）
场景：要选择最小的k个数。
做法：【构造大顶堆（一共k个数）】，每次取数组中剩余数与 【堆顶的元素（k个数中的最大数）】 进行比较。如果新数比堆顶元素小，则删除堆顶元素，并添加这个新数到堆中；如果新数比堆顶元素大，则说明堆中的k个数都比新数小，所以不用进行操作。

# 大量数据中找出不重复整数

### 题目描述

在 2 亿个整数中找出不重复的整数，内存不足以容纳这 2 亿个整数。

### 解答思路

#### 方法一：分治法

与前面的题目方法类似，先将 2 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

#### 方法二：位图法

位图是指用一个或多个 bit 来标记某个元素对应的值。因为使用bit为单位存储数据可以大大节省存储空间。对于这道题，我们可以用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

2亿个整数，总共所需内存为

```shell
2亿个整数 × 4字节/整数 = 8亿字节
1GB等于1,073,741,824字节。
8亿字节 ÷ 1,073,741,824字节/GB ≈ 0.745GB。
```

因此，如果此题的可用内存超过 0.75GB 时，可以采用位图法。
遍历 2 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

# 大量数据中判断一个数是否存在

### 题目描述

给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

### 解答思路

#### 方法一：分治法

分治法解决，与前面类似，不再赘述。

#### 方法二：位图法

40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。
我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

# 大量数据中找出中位数

### 题目描述

从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。

### 解答思路

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN)。这里使用其他方法。

#### 方法一：双堆法

维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数**小于等于**小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。
若数据总数为**偶数**，当这两个堆建好之后，**中位数就是这两个堆顶元素的平均值**。当数据总数为**奇数**时，根据两个堆的大小，**中位数一定在数据多的堆的堆顶**。
以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法**适用于数据量较小的情况**。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了。

#### 方法二：分治法

分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。
顺序读取这 5 亿个数字，对于读取到的数字 number，如果它对应的二进制中最高位为1，则把这个数字写到 file(1) 中，否则写入 file(0) 中。通过这一步，可以把这 5 亿个数划**分为两部分**，而且 file(0) 中的数都大于 file(1) 中的数（最高位是符号位）。
划分之后，可以非常容易地知道中位数是在 file(0) 还是 file(1) 中。假设 file(1) 中有 1 亿个数，那么中位数一定在 file(0) 中，且是在 file(0) 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。
由于5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 file(1) 有一亿个数，那么中位数就是 file(0) 中从第 1.5 亿个数开始的两个数求得的平均值。
对于 file(0) 可以用次高位的二进制继续将文件一分为二，如此划分下去，**直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。**
当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

# 大文件查询频度排序

### 题目描述

有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。

### 解答思路

如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。

#### 方法一：HashMap 法

如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。

#### 方法二：分治法

分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。我们顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。
接着对所有文件按照 query 的次数进行排序，这里可以使用**归并排序**（由于无法把所有 query 都读入内存，因此需要使用**外排序**）。

**如何理解外排序？**
例如，有一个含有 10000 个记录的文件，但是内存的可使用容量仅为 1000 个记录，毫无疑问需要使用外部排序算法，具体分为两步：

- 将整个文件其等分为 10 个临时文件（每个文件中含有 1000 个记录），然后将这 10 个文件依次进入内存，采取适当的内存排序算法对其中的记录进行排序，将得到的有序文件（初始归并段）移至外存。
- 对得到的 10 个初始归并段进行如图 1 的两两归并，直至得到一个完整的有序文件。
  ![image.png](https://s2.loli.net/2023/10/15/tFekd3o6zRa85m4.png)
**注意：** 在实际归并的过程中，由于内存容量的限制不能满足同时将 2 个归并段全部完整的读入内存进行归并，只能不断地取 2 个归并段中的每一小部分进行归并，通过不断地读数据和向外存写数据，直至 2 个归并段完成归并变为 1 个大的有序文件。